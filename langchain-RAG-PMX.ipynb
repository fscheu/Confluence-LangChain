{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Retrieval Augmented Generation (RAG) App\n",
    "Este archivo es una prueba siguiendo el tutorial descripto en la página: https://python.langchain.com/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "```console\n",
    "py venv -m venv\n",
    ".\\\\venv\\scripts\\activate\n",
    "pip install --quiet --upgrade langchain langchain-community langchain-chroma\n",
    "pip install -qU langchain-openai\n",
    "pip install bs4\n",
    "pip install lxml\n",
    "pip install pypdf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith\n",
    "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = utils.config[\"LANG\"][\"LANGCHAIN_TRACING_V2\"]\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = utils.config[\"LANG\"][\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG-PMX\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Indexing: Load\n",
    "We need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Documents. A Document is an object with some page_content (str) and metadata (dict).\n",
    "\n",
    "In this case we’ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class “post-content”, “post-title”, or “post-header” are relevant, so we’ll remove all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\autoevaluacion.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\carga-plan-trabajo.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\dashboard.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\desarrollo.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\evaluacion.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\evaluaciones.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\feedback-enviado.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\feedback-recibido.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\feedback.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\index.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\mi-equipo.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\mi-perfil.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\objetivos-competencias.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\potencial.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\reportes.html\n",
      "file path: C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual\\3.02. Gestión de Performance.pdf\n",
      "2068\n",
      "\n",
      "\n",
      "\n",
      "Autoevaluación - PMX - Guía de usuarios\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Guía de usuario PMX\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Introducción Dashboard Mi equipo\n",
      "\n",
      "Evaluaciones\n",
      "\n",
      "Introducción\n",
      "Objetivos y Competencias\n",
      "\n",
      "Carga del plan de trabajo\n",
      "Autoevaluación\n",
      "Evaluación\n",
      "\n",
      "\n",
      "Feedback\n",
      "\n",
      "Feedback Recibido\n",
      "Feedback Enviado\n",
      "\n",
      "\n",
      "Potencial\n",
      "Desarrollo\n",
      "\n",
      "\n",
      "Reportes Mi Perfil \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "En esta página\n",
      "\n",
      "\n",
      "Autoevaluación de objetivos\n",
      "Instancia de autoevaluación\n",
      "Autoevaluación de competencias\n",
      "Competencias & Comportamientos\n",
      "Comentarios sobre competencias\n",
      "Comentarios de \n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from langchain.document_loaders import BSHTMLLoader, PyPDFLoader\n",
    "\n",
    "# Ruta con un patrón de búsqueda para archivos HTML\n",
    "html_files = glob(\"C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es/*.html\")\n",
    "pdf_files = glob(\"C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/*.pdf\")\n",
    "\n",
    "# Cargar y procesar cada archivo\n",
    "documents = []\n",
    "for file_path in html_files:\n",
    "    print(f\"file path: {file_path}\")\n",
    "    loader = BSHTMLLoader(file_path, open_encoding=\"utf-8\")\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    print(f\"file path: {file_path}\")\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents.extend(loader.load())\n",
    "# Ahora `documents` contiene todos los documentos cargados.\n",
    "print(len(documents[0].page_content))\n",
    "\n",
    "print(documents[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "927\n",
      "{'source': 'C:/Users/baiscf/repos_local/Confluence-LangChain/PMX_Manual/tenaris/es\\\\dashboard.html', 'title': 'Dashboard - PMX - Guía de usuarios', 'start_index': 859}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(len(all_splits))\n",
    "print(len(all_splits[0].page_content))\n",
    "print(all_splits[10].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "# from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# embed = AzureOpenAIEmbeddings(\n",
    "#     model=utils.config[\"EMB\"][\"MODEL\"],\n",
    "#     azure_endpoint=utils.config[\"EMB\"][\"ENDPOINT\"],\n",
    "#     api_key=utils.config[\"EMB\"][\"API_KEY\"],\n",
    "#     api_version=utils.config[\"EMB\"][\"API_VERSION\"],\n",
    "# )\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = utils.config[\"EMB\"][\"OPENAI_API_KEY\"]\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Es importante destacar que para accionar en esta etapa se debe tener todos los objetivos aprobados. \n",
      " \n",
      "\n",
      "Una vez realizada la autoevaluación se verá destacada en la escala (1) y, al colapsar, en el margen superior derecho (2).\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Instancia de autoevaluación\n",
      "Una vez que realices la entrevista de feedback con el supervisor y te hayan habilitado la evaluación, podrás visualizar tu autoevaluación (1) en el extremo inferior izquierdo y la evaluación del jefe (2) en la escala. \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Autoevaluación de competencias\n",
      "Desde esta sección se podrá realizar la autoevaluación de las competencias. \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Competencias \t& Comportamientos\n",
      "Al expandir cada una de las competencias se podrá visualizar su definición y los comportamientos asociados con una breve encuesta para completar, los cuales servirán de guía para realizar la evaluación en la escala del 1 al 5. \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Comentarios sobre competencias\n",
      "Se podrán dejar comentarios en cada competencia que luego visualizará el responsable.\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Como puedo comparar la autoevaluacion y la evaluacion?\")\n",
    "\n",
    "print(len(retrieved_docs))\n",
    "\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = utils.config[\"LLM\"][\"ENDPOINT\"]\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = utils.config[\"LLM\"][\"API_KEY\"]\n",
    "# os.environ[\"AZURE_OPENAI_API_VERSION\"] = utils.config[\"LLM\"][\"API_VERSION\"]\n",
    "# os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = utils.config[\"LLM\"][\"DEPLOYMENT\"]\n",
    "\n",
    "# from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# llm = AzureChatOpenAI(\n",
    "#     azure_endpoint=utils.config[\"LLM\"][\"ENDPOINT\"],\n",
    "#     azure_deployment=utils.config[\"LLM\"][\"DEPLOYMENT\"],\n",
    "#     openai_api_version=utils.config[\"LLM\"][\"API_VERSION\"],\n",
    "# )\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = utils.config[\"LLM\"][\"OPENAI_API_KEY\"]\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use a prompt for RAG that is checked into the LangChain prompt hub (here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "example_messages\n",
    "\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use the LCEL Runnable protocol to define the chain, allowing us to\n",
    "\n",
    "pipe together components and functions in a transparent way\n",
    "automatically trace our chain in LangSmith\n",
    "get streaming, async, and batched calling out of the box.\n",
    "Here is the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para comparar la autoevaluación y la evaluación, puedes visualizar ambas en la escala de desempeño donde la autoevaluación se muestra en el extremo inferior izquierdo y la evaluación del jefe en la parte superior derecha. La autoevaluación permite al colaborador autoevaluarse en función de sus objetivos y competencias, mientras que la evaluación del jefe se centra en el desempeño integral del colaborador. Además, se pueden dejar comentarios en cada competencia que serán considerados en ambas evaluaciones."
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"Como puedo comparar la autoevaluacion y la evaluacion?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
